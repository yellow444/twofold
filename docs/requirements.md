# Data Processing and Analytics Requirements

## Supported Source Formats and Ingestion Constraints

### CSV
- **Expected structure:** UTF-8 encoded, comma-separated with a header row describing canonical fields (flight_id, start_time, end_time, region_code, coordinates, etc.).
- **Parsing constraints:**
  - Delimiter must be a literal comma; quoted fields are double quoted according to RFC 4180.
  - Line endings may be `\n` or `\r\n`; carriage returns are normalized during ingest.
  - Timestamp fields are parsed as ISO 8601 and normalized to UTC; missing offsets default to source region local time before conversion.
  - Null values are represented as empty strings or the literal `NULL`.
  - Files larger than 200 MB must be streamed to avoid exceeding container memory limits.
- **Ingestion flow:** Agent.Ingest reads from MinIO/S3, applies schema validation, trims whitespace, and emits normalized Parquet/CSV artifacts before loading PostgreSQL staging tables.

### XLS / XLSX
- **Expected structure:** Workbook containing a single data sheet named `Flights` with header row; additional sheets are ignored.
- **Parsing constraints:**
  - Cells must be typed; mixed numeric/text columns trigger a validation warning.
  - Merged cells are unsupported; file must be flattened prior to upload.
  - Date/time cells converted using workbook TZ metadata; absent metadata assumes Moscow time (Europe/Moscow) prior to UTC conversion.
  - Maximum 100,000 rows per sheet to keep ingestion within SLA.
- **Ingestion flow:** Sheet exported to CSV in-memory via `openpyxl`/`pyxlsb` readers, then reuses CSV pipeline rules.

### PDF
- **Expected structure:** Tabular PDF exports generated by Rosaviatsiya template with consistent column order and table borders.
- **Parsing constraints:**
  - Processed via `camelot`/`tabula` in lattice mode; requires vector-based tables (no scanned images).
  - Text extraction assumes embedded fonts; rasterized pages are rejected with `quality_status=FAIL`.
  - Header/footer noise removed using regex filters; multi-page tables concatenated based on detected column alignment.
  - Manual review required if column count differs from canonical schema.
- **Ingestion flow:** Extracted tables materialized to intermediate CSV, then validated and loaded identically to CSV ingest.

### HTML
- **Expected structure:** Static HTML reports containing a single `<table>` element with canonical headers.
- **Parsing constraints:**
  - HTML must be well-formed; parser uses `BeautifulSoup` with `lxml` backend.
  - Cells containing nested markup are flattened to text; hyperlinks preserved as separate metadata columns when present.
  - Locale-specific decimal separators are normalized (`,` → `.`).
  - Timezone metadata inferred from `<meta name="report-tz">` when provided; otherwise uses Europe/Moscow.
- **Ingestion flow:** Table converted to pandas DataFrame, serialized to Parquet and CSV, and forwarded to staging ingestion path.

## Analytics Metrics and Aggregation Rules

### Core Metrics
- **Flight count:** Number of unique flights after deduplication.
- **Total duration:** Sum of flight durations in minutes (end_time − start_time) converted to hours for reporting dashboards.
- **Average duration:** Mean duration per flight in minutes; excludes records with missing start/end timestamps.
- **Month-over-month dynamics:** Percentage change of flight count and total duration between consecutive months within the same year.
- **Top-N regions:** Ranking of regions by flight count and total duration; default N=5 with configurability.

### Aggregation Policies
- **Time zones:**
  - All timestamps normalized to UTC before storage.
  - Aggregations executed in UTC; UI conversion to local zones handled downstream.
- **Grouping keys:**
  - Primary keys: `region_id`, `month` (derived from start_time in UTC), `vehicle_category` (if present).
  - Additional breakdowns: `operator_type`, `payload_type`, and `flight_purpose` when fields are populated.
- **Deduplication assumptions:**
  - Unique flight identity is `(flight_id, start_time, region_id)`.
  - On duplicates, record with latest `ingested_at` supersedes earlier entries; superseded rows excluded from aggregates.
  - Missing `flight_id` values trigger surrogate key generation based on `(start_time, coordinates_hash)` with quality flag `surrogate_id=true`.
- **Data quality filters:**
  - Durations ≤ 0 or > 24 hours discarded from metrics and logged for QA review.
  - Records outside the reporting year ignored in monthly aggregates.

## SLA, Environment, and Export Requirements

- **Processing SLA:** Ability to ingest, normalize, and aggregate ≤10,000 records within 5 minutes on reference hardware; monitoring metric `etl_duration_seconds` enforces SLA breach alerts.
- **Target environment prerequisites (Linux):**
  - Ubuntu 22.04 LTS (or compatible) with Docker 24+, docker-compose plugin, Python 3.11+, .NET SDK 8.0, Node.js 20+, Java 17+ (for PDF extraction), and system packages `libxml2`, `libxslt`, `libpoppler-dev`, `libmagic1`.
  - Network access to internal PostgreSQL/PostGIS, MinIO, Keycloak, and optional ClickHouse services defined in `infra/docker-compose.yml`.
  - Locale set to `en_US.UTF-8` to ensure consistent parsing and export formatting.
- **Required export formats:** Backend must provide synchronous exports for PNG, JPEG (chart snapshots), JSON (raw aggregates), CSV, and XLSX. PNG/JPEG rendering uses headless Chromium; CSV/XLSX generated from aggregate views; JSON returns structured API payloads.

## Acceptance Criteria / Definition of Done

- ✅ Ingestion agents successfully parse and load CSV, XLS/XLSX, PDF, and HTML samples adhering to constraints above, with validation reports stored alongside dataset versions.
- ✅ Deduplication logic enforces `(flight_id, start_time, region_id)` uniqueness and flags surrogate IDs when generated.
- ✅ Aggregated metrics (flight count, total/average duration, month-over-month deltas, Top-N regions) reproduce expected sample benchmarks and are accessible via `/stats` API with UTC-based grouping.
- ✅ Export endpoints `/export?format=csv|xlsx|json|png|jpeg` return valid artifacts for sample datasets within SLA.
- ✅ Monitoring dashboards display `etl_duration_seconds`, `quality_fail_count`, and alerting rules for SLA breaches.
- ✅ Documentation reflects requirements traceability: implementation tasks reference this document in Jira/issue tracker, and QA test cases map each acceptance criterion to automated or manual tests.

